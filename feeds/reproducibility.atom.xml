<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Ian Hawke</title><link href="http://IanHawke.github.io/blog/" rel="alternate"></link><link href="http://IanHawke.github.io/blog/feeds/reproducibility.atom.xml" rel="self"></link><id>http://IanHawke.github.io/blog/</id><updated>2016-05-19T12:00:00+01:00</updated><entry><title>How do you choose a reproducibility metric?</title><link href="http://IanHawke.github.io/blog/how-do-you-choose-a-reproducibility-metric.html" rel="alternate"></link><updated>2016-05-19T12:00:00+01:00</updated><author><name>Ian Hawke</name></author><id>tag:IanHawke.github.io,2016-05-19:blog/how-do-you-choose-a-reproducibility-metric.html</id><summary type="html">&lt;p&gt;Some papers are effortlessly brilliant. Some show their brilliance by lifting the lid on just how much work it is to do it &lt;em&gt;right&lt;/em&gt;. Oliver Mesnard and Lorena Barba just &lt;a href="http://arxiv.org/abs/1605.04339"&gt;released a preprint&lt;/a&gt; whose title, "&lt;em&gt;Reproducible and replicable CFD: it's harder than you think&lt;/em&gt;", shows it firmly belongs in the latter camp. Combine that with the &lt;a href="https://github.com/barbagroup/snake-repro"&gt;detailed repository&lt;/a&gt; containing all the code needed to do the work, there's enough information to lose a couple of days.&lt;/p&gt;
&lt;p&gt;What most interests me is the criteria chosen for successful reproducibility. I've argued before about what is &lt;a href="http://ianhawke.github.io/blog/close-enough.html"&gt;close enough&lt;/a&gt; when comparing a numerical simulation to theoretical expectations, and about &lt;a href="http://ianhawke.github.io/blog/the-image-is-the-simulation.html"&gt;the minimum I'd like to be able to reproduce&lt;/a&gt;. But this Mesnard and Barba's paper is far more complex, and is code versus code comparison with experimental backup, not code versus theory. The criteria discussed in the introduction include&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A previous experimental study had already shown that the lift coefficient of a snake cross section in a wind tunnel gets an extra oomph of lift at 35 degrees angle-of-attack. Our simulations showed the same feature in the plot of lift coefficient. Many detailed observations of the wake (visualized from the fluid-flow solution in terms of the vorticity field in space and time) allowed us to give an explanation of the mechanism providing extra lift.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is a much more complex than comparing a single number. Hence the paper contains statements such as&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Is it acceptable as a replication study? We think yes, but this is a judgement call.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;and&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;we make a judgement call that this result does indeed pass muster as a replication of our previous study.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How is this judgement call made? Is this art or science? What needs to be reproduced, and how closely?&lt;/p&gt;
&lt;h2&gt;Samurai and Ninja&lt;/h2&gt;
&lt;p&gt;To think further about this, let's look at a totally different study: binary black holes. Long before the &lt;a href="http://www.ligo.org/science/Publication-GW150914/index.php"&gt;GW150914 observation&lt;/a&gt; there was a lot of concern about the accuracy of numerical simulations of black holes. After a lot of effort, gravitational wave signals were finally being computed from numerical simulations. With no experiment to compare against, and with significant differences in both the form of the equations and the numerical methods used, the possibility that some, if not all, of the simulations were simply wrong was a big worry.&lt;/p&gt;
&lt;p&gt;The &lt;a href="http://journals.aps.org/prd/abstract/10.1103/PhysRevD.79.084025"&gt;Samurai&lt;/a&gt; (&lt;a href="http://arxiv.org/abs/0901.2437"&gt;arXiv link&lt;/a&gt;) and &lt;a href="http://iopscience.iop.org/article/10.1088/0264-9381/26/16/165008/meta"&gt;Ninja&lt;/a&gt; (&lt;a href="http://arxiv.org/abs/0901.4399"&gt;arXiv link&lt;/a&gt;) were the result. In both cases the key question was "Does it make any difference to a &lt;em&gt;detection&lt;/em&gt; if we use one numerical result rather than another?". There was a clear metric, given by the "best mismatch" between predicted waveforms, after folding in the expected noise from the experiment. In the Ninja paper waveforms predicted by different codes were directly compared; in the Samurai paper the link to the full data analysis pipelines are made, to see what useful information can be pulled out.&lt;/p&gt;
&lt;p&gt;The result was simple: for current detectors, &lt;em&gt;all&lt;/em&gt; the numerical results were effectively indistinguishable. The difficulty of doing the experiment at all constrained the results so that only the grossest, most qualitative features of the simulations stood out from the noise. This reproducibility metric worked, but hid many of the interesting differences between the simulations. However, without any experiment that could show the differences, is there a point in going further?&lt;/p&gt;
&lt;h2&gt;Questioning the metric&lt;/h2&gt;
&lt;p&gt;In the Mesnard and Barba case, they have broadly specified two "metrics":&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the variation of the lift coefficient with angle-of-attack;&lt;/li&gt;
&lt;li&gt;features in the wake explaining the lift mechanism.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The simplest numerical metric is the variation of the lift coefficient, and figures 3, 6, and 8 of their paper show how the time averaged lift coefficients compare across simulations and experiment. What is more interesting to me is the way that Mesnard and Barba challenge this metric, by looking at the variation of the instantaneous lift coefficients (figures 4, 7, 9, and 11), discussing the impact of some of the arbitrary choices (such as the time period over which the averaging is done), and the simulation issues that could cause this. This links to the second metric: by exploring the wake features they can explain issues with the simulations, including meshing and boundary condition problems. These are clear from some of the figures, but difficult to quantify.&lt;/p&gt;
&lt;p&gt;This shows a clear difference from the Samurai and Ninja papers. The first metric is there to reproduce the experimental result. If the theory is certain and the mechanism clear, there's nothing more to say. But for the Mesnard and Barba reproduction, they wish in addition to test the &lt;em&gt;explanation&lt;/em&gt; of the results. This means digging deeper into the details of the simulation results, which in turn shows differences in the lift coefficients. Their reproduction is going beyond the numerical matching of an experimental result and into the physical prediction that a detailed numerical simulation can give.&lt;/p&gt;
&lt;h2&gt;Lessons&lt;/h2&gt;
&lt;p&gt;Mesnard and Barba detail their lessons learned, but to me this paper brings home yet another point. Reproducibility has a number of technical aspects which can be extremely hard, as beautifully illustrated by this paper. However, it also has the human, non-technical aspect of choosing &lt;em&gt;what&lt;/em&gt; needs to be reproduced: what is the metric? Finally, there's the issue of how that metric will change over time, as the depth of the reproduction goes from the general features that an experiment will catch to the qualitative details that "explain" why something happens.&lt;/p&gt;</summary><category term="Reproducibility"></category></entry><entry><title>The image is the simulation</title><link href="http://IanHawke.github.io/blog/the-image-is-the-simulation.html" rel="alternate"></link><updated>2015-06-01T09:00:00+01:00</updated><author><name>Ian Hawke</name></author><id>tag:IanHawke.github.io,2015-06-01:blog/the-image-is-the-simulation.html</id><summary type="html">&lt;p&gt;Reproducibility in science is of obvious importance, and the number of retractions and papers tackling this have highlighted the distance we still have to go. There's also been a lot of work on producing tools that make reproducibility easier, and a push to embed this through training. At the very least, the goals of the Recomputation Manifesto look to be closer now.&lt;/p&gt;
&lt;p&gt;My thought here is about &lt;em&gt;what&lt;/em&gt; is reproduced, not &lt;em&gt;how&lt;/em&gt;. In particular, what is it that represents a computer simulation?&lt;/p&gt;
&lt;h2&gt;Papers versus images&lt;/h2&gt;
&lt;p&gt;To most academics, the currency is papers and grants. As no funding agency has yet required a reproducible grant proposal, a lot of focus has gone into reproducible papers. At the same time, there's been a lot of comment that "the academic paper is dead or dying", or at least won't be fit for purpose in the near future. There's a minor conflict here, which may be one reason why reproducible &lt;em&gt;papers&lt;/em&gt; have gained less traction than reproducible &lt;em&gt;code&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Leaving that aside, to me the paper doesn't &lt;em&gt;represent&lt;/em&gt; the simulation. &lt;em&gt;My&lt;/em&gt; simulations build on detailed thought, carefully constructed code, and the best analysis I can produce of the complex resulting data. With the same level of seriousness, &lt;em&gt;your&lt;/em&gt; simulations are impressive work, but with weird voodoo magic I don't understand, and &lt;em&gt;their&lt;/em&gt; simulations are just a fancy visualization package on top of a random number generator.&lt;/p&gt;
&lt;p&gt;The serious point: to me, a simulation is&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the problem being studied;&lt;/li&gt;
&lt;li&gt;the questions we're trying to answer;&lt;/li&gt;
&lt;li&gt;how widely applicable, and accurate, we think our answers are.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Most of the detail that covers these points is summed up not in the paper, but in the figures and figure captions. The paper is "just" the framework and prose glue that holds the figures together.&lt;/p&gt;
&lt;p&gt;A computer science analogy comes from Brooks' quote adapted for Object Oriented programming by Raymond:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Show me your code without your [classes] and I won't understand. Show me your classes without your code, and I won't need to see the code.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I believe the analogy holds for simulation papers and images. A simulation paper without images will either be unintelligible or far too long to read. A good set of images can show the ideas and key content behind the paper without needing the paper.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Result of a pyro simulation" src="https://raw.githubusercontent.com/IanHawke/im2sim/master/example_figure/plot.png" /&gt;&lt;/p&gt;
&lt;p&gt;So, &lt;strong&gt;the image is the simulation&lt;/strong&gt;. And this is a &lt;em&gt;big&lt;/em&gt; problem.&lt;/p&gt;
&lt;h2&gt;The image as orphan&lt;/h2&gt;
&lt;p&gt;Hyperbole aside, images - unlike in-depth academic arguments - aren't confined to papers. Everywhere the work might be advertised - talks, websites, grant proposals and more - the image is doing much of the work: representing the simulation, standing in for the months of code wrangling and contentious analysis. That image that appeared on a student poster might, in a few years, be the key figure selling a grant proposal, or illustrating a research lab's work, or holding together a chapter of a thesis, or representing to another scientist all that is wrong (or right) about another field of research. How many problems result from people arguing about what they &lt;em&gt;think&lt;/em&gt; a result means, thanks to arguing with the figure out of context, and the original context (code and analysis) can't easily be reproduced?&lt;/p&gt;
&lt;p&gt;There is a link to be made with the amazing "artists' impressions" that NASA produces, often to illustrate new observational results. They're incredible feats that condense the state of the art science into easily grasped images. However, it's always clear that these aren't real. When the image is from a computer simulation, which is meant to be a cutting-edge model of how things "really" are, then it can have disproportionate impact on scientists and non-scientists when pulled out of context.&lt;/p&gt;
&lt;h2&gt;im2sim&lt;/h2&gt;
&lt;p&gt;One way to tackle this is to make the image &lt;em&gt;really&lt;/em&gt; represent the simulation, by making it possible to &lt;em&gt;replicate&lt;/em&gt; the full simulation (code, parameter files, data analysis scripts and all) just from the figure. Note that I'm using &lt;a href="http://reproduciblescience.blogspot.co.uk/2015/05/rebility.html"&gt;this definition&lt;/a&gt; of replicability versus reproducibility, even though I've loosely used reproducibility throughout this post. If the details are embedded in the figure metadata, in such a way that the simulation can be &lt;em&gt;easily&lt;/em&gt; recovered, then it would be possible to link the figure with its original context in a reproducible way.&lt;/p&gt;
&lt;p&gt;Here's an example, using the figure at the top of the page. To reproduce this figure, you'll need the &lt;code&gt;im2sim&lt;/code&gt; code from &lt;a href="http://ianhawke.github.com/im2sim"&gt;this github repository&lt;/a&gt;, which is a simple &lt;code&gt;python&lt;/code&gt; script, and the figure itself, which you can just "save as" directly from this page.&lt;/p&gt;
&lt;p&gt;The idea I've used here is the simplest hack I could think of that would give the entire simulation. The metadata of the image contains the SHA1 hash of a &lt;code&gt;docker&lt;/code&gt; container stored on &lt;a href="http://hub.docker.com"&gt;dockerhub&lt;/a&gt;. This container includes the code to produce it (in this case, &lt;code&gt;pyro2&lt;/code&gt;, which is Mike Zingale's code illustrating a number of CFD techniques) and a Makefile. &lt;code&gt;im2sim reproduce&lt;/code&gt; will (assuming you have &lt;code&gt;docker&lt;/code&gt; installed) pull the container, then tell you the command you need to run to reproduce the images, or the command to use to inspect the content of the container.&lt;/p&gt;
&lt;p&gt;The alternative use of the script can be illustrated once you have the container associated with the image. &lt;code&gt;im2sim mark&lt;/code&gt; can be called &lt;em&gt;on the container&lt;/em&gt;. This will run the Makefile in the container, produce the images, and then edit the metadata of the images to include the SHA1 hash. Those images can then be distributed, and anyone can reproduce the simulation using just the image and the &lt;code&gt;im2sim&lt;/code&gt; script.&lt;/p&gt;
&lt;h2&gt;Is this for everyone?&lt;/h2&gt;
&lt;p&gt;Let me rephrase that: should you download a script and image from the internet and run it on your machine, knowing that effective use of &lt;code&gt;docker&lt;/code&gt; requires root permission on your machine? The answer is obviously, "&lt;em&gt;No, you should not use this code!&lt;/em&gt;".&lt;/p&gt;
&lt;p&gt;However, building something like this into your workflow &lt;em&gt;would&lt;/em&gt; be a good idea. If you've already got a reproducible workflow then adding the metadata that gets back to the last analysis step producing the figures would be enough. Adding a link to, for example, a Jupyter notebook that produces the figure whilst explaining the analysis would be better. Any way that works!&lt;/p&gt;
&lt;h6&gt;Acknowledgements&lt;/h6&gt;
&lt;p&gt;The code to add the metadata to image files was &lt;a href="https://github.com/dfm/savefig"&gt;taken from this code&lt;/a&gt;. It monkey-patches &lt;code&gt;matplotlib&lt;/code&gt; to add the git hash of the local directory to the figure produced. This builds the reproducibility aspect into the workflow more naturally, and would be a better approach in many ways - it ensures the image automatically gets the metadata - but unlike the docker approach it doesn't contain the full workflow.&lt;/p&gt;
&lt;p&gt;The setup of the &lt;code&gt;docker&lt;/code&gt; container comes from &lt;a href="http://stackoverflow.com/a/26547845/3112941"&gt;this stackoverflow answer&lt;/a&gt; which references &lt;a href="https://github.com/marmelab/make-docker-command/blob/master/Makefile"&gt;this Makefile&lt;/a&gt;. I've taken the approach but switched to python simplly to use one script for the lot.&lt;/p&gt;
&lt;p&gt;Mike Zingale's &lt;code&gt;pyro2&lt;/code&gt; code covers a lot more than I've shown here. It's an excellent learning resource, especially used in conjunction with &lt;a href="https://github.com/Open-Astrophysics-Bookshelf/numerical_exercises"&gt;his detailed notes&lt;/a&gt;.&lt;/p&gt;</summary><category term="Reproducibility"></category></entry></feed>